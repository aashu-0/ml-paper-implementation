{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:45.107509Z",
     "iopub.status.busy": "2025-02-04T09:44:45.107204Z",
     "iopub.status.idle": "2025-02-04T09:44:50.414166Z",
     "shell.execute_reply": "2025-02-04T09:44:50.413465Z",
     "shell.execute_reply.started": "2025-02-04T09:44:45.107486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:50.415489Z",
     "iopub.status.busy": "2025-02-04T09:44:50.415189Z",
     "iopub.status.idle": "2025-02-04T09:44:56.271321Z",
     "shell.execute_reply": "2025-02-04T09:44:56.270638Z",
     "shell.execute_reply.started": "2025-02-04T09:44:50.415470Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                          train=True,\n",
    "                                          download=True,\n",
    "                                          transform=train_transform,)\n",
    "test_data = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                         train = False,\n",
    "                                         download = True,\n",
    "                                         transform = test_transform)\n",
    "\n",
    "                                            \n",
    "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS)\n",
    "test_loader = DataLoader(test_data, batch_size = BATCH_SIZE, shuffle = False, num_workers = NUM_WORKERS)\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:56.273106Z",
     "iopub.status.busy": "2025-02-04T09:44:56.272832Z",
     "iopub.status.idle": "2025-02-04T09:44:56.279057Z",
     "shell.execute_reply": "2025-02-04T09:44:56.278277Z",
     "shell.execute_reply.started": "2025-02-04T09:44:56.273083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:56.280454Z",
     "iopub.status.busy": "2025-02-04T09:44:56.280146Z",
     "iopub.status.idle": "2025-02-04T09:44:56.292969Z",
     "shell.execute_reply": "2025-02-04T09:44:56.292269Z",
     "shell.execute_reply.started": "2025-02-04T09:44:56.280419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# patch embedding\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                img_size=224,\n",
    "                patch_size = 16,\n",
    "                in_channels = 3,\n",
    "                embedding_dim = 768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.num_patches = (img_size//patch_size)**2\n",
    "\n",
    "        # 1. images -> patches -> linear proj on each patch\n",
    "        self.proj = nn.Conv2d(in_channels = in_channels,\n",
    "                             out_channels = embedding_dim,\n",
    "                             kernel_size = patch_size,\n",
    "                             stride = patch_size,\n",
    "                             padding=0)\n",
    "        self.flatten = nn.Flatten(2,3)\n",
    "        # 2. adding cls(class) token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embedding_dim),\n",
    "                                     requires_grad= True) # by default-> true\n",
    "        # 3. positional embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches+1,embedding_dim),\n",
    "                                         requires_grad= True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2,3)\n",
    "        x = x.transpose(1,2)\n",
    "        # print(f'Patched image shape: {x.shape}')\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        cls_token = self.cls_token.expand(batch_size, -1,-1)\n",
    "        \n",
    "        # prepend cls token\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        # print(f'patch embedding with cls token shape: {x.shape}')\n",
    "\n",
    "        # add pos\n",
    "        x = x + self.pos_embedding\n",
    "        # print(f'patch + cls + pos shape: {x.shape}')\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:56.294209Z",
     "iopub.status.busy": "2025-02-04T09:44:56.293879Z",
     "iopub.status.idle": "2025-02-04T09:44:57.063130Z",
     "shell.execute_reply": "2025-02-04T09:44:57.062075Z",
     "shell.execute_reply.started": "2025-02-04T09:44:56.294179Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "# get an image\n",
    "batch_img, batch_label = next(iter(train_loader))\n",
    "\n",
    "# single img\n",
    "img, label = batch_img[0], batch_label[0]\n",
    "print(f'Input image shape: {img.shape}')\n",
    "\n",
    "\n",
    "patchin = PatchEmbedding(img_size=224,\n",
    "                        patch_size = 16,\n",
    "                        in_channels = 3,\n",
    "                        embedding_dim = 768)\n",
    "# add batch dim\n",
    "img = img.unsqueeze(0)\n",
    "print(f'Input image shape after adding batch dim: {img.shape}')\n",
    "\n",
    "output_from_patch_embedding = patchin(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:57.064645Z",
     "iopub.status.busy": "2025-02-04T09:44:57.064364Z",
     "iopub.status.idle": "2025-02-04T09:44:57.309617Z",
     "shell.execute_reply": "2025-02-04T09:44:57.308760Z",
     "shell.execute_reply.started": "2025-02-04T09:44:57.064621Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot\n",
    "img_ = img.squeeze(0)\n",
    "img_ = img_.permute(1,2,0)\n",
    "\n",
    "plt.figure(figsize = (6,3))\n",
    "plt.imshow(img_);\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspired from [code](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:57.311035Z",
     "iopub.status.busy": "2025-02-04T09:44:57.310670Z",
     "iopub.status.idle": "2025-02-04T09:44:57.317719Z",
     "shell.execute_reply": "2025-02-04T09:44:57.316880Z",
     "shell.execute_reply.started": "2025-02-04T09:44:57.311003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# multi head attention\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim = 768,\n",
    "                 num_heads = 12,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim =  embedding_dim // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(embedding_dim, embedding_dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # batch_size, num_patches+1, embedding_dim = x.shape\n",
    "        batch_size, num_patches, embedding_dim = x.shape\n",
    "        # print(f'input to multi head attention: {x.shape}')\n",
    "\n",
    "        qkv = self.qkv_proj(x).reshape(batch_size, num_patches, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0,2,1,3) # [batch_size, num_heads, num_patches, dim]\n",
    "        q,k,v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # scaled dot-product\n",
    "        attn = (torch.matmul(q, k.transpose(-2,-1)))/ math.sqrt(self.head_dim)\n",
    "        \n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = torch.matmul(attn, v)\n",
    "        x = x.permute(0,2,1,3) # [batch_size, num_patches, num_heads, dim]\n",
    "        x = x.reshape(batch_size, num_patches, embedding_dim)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:57.320602Z",
     "iopub.status.busy": "2025-02-04T09:44:57.320375Z",
     "iopub.status.idle": "2025-02-04T09:44:57.392882Z",
     "shell.execute_reply": "2025-02-04T09:44:57.392089Z",
     "shell.execute_reply.started": "2025-02-04T09:44:57.320583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "attentionin = MultiHeadAttention(embedding_dim = 768,\n",
    "                                 num_heads = 12)\n",
    "\n",
    "output_from_multi_head_attention = attentionin(output_from_patch_embedding)\n",
    "print(f'output_from_multi_head_attention shape: {output_from_multi_head_attention.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:57.394467Z",
     "iopub.status.busy": "2025-02-04T09:44:57.394187Z",
     "iopub.status.idle": "2025-02-04T09:44:57.399083Z",
     "shell.execute_reply": "2025-02-04T09:44:57.398293Z",
     "shell.execute_reply.started": "2025-02-04T09:44:57.394445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim=768,\n",
    "                 mlp_size=3072,\n",
    "                 dropout=0.):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mlp_size = mlp_size\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features= embedding_dim,\n",
    "                     out_features = mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p = dropout),\n",
    "            nn.Linear(in_features = mlp_size,\n",
    "                     out_features = embedding_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:57.400172Z",
     "iopub.status.busy": "2025-02-04T09:44:57.399836Z",
     "iopub.status.idle": "2025-02-04T09:44:57.494237Z",
     "shell.execute_reply": "2025-02-04T09:44:57.493502Z",
     "shell.execute_reply.started": "2025-02-04T09:44:57.400139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mlpin = MLP(embedding_dim=768,\n",
    "            mlp_size=3072,\n",
    "            dropout= 0.1)\n",
    "\n",
    "output_through_mlp_layer = mlpin(output_from_multi_head_attention)\n",
    "print(f'Output through mlp layer shape: {output_through_mlp_layer.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:57.495192Z",
     "iopub.status.busy": "2025-02-04T09:44:57.494965Z",
     "iopub.status.idle": "2025-02-04T09:44:57.500870Z",
     "shell.execute_reply": "2025-02-04T09:44:57.500046Z",
     "shell.execute_reply.started": "2025-02-04T09:44:57.495172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TransformerBlock\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "               embedding_dim = 768,\n",
    "               num_heads = 12,\n",
    "               mlp_size=3072,\n",
    "               mlp_dropout = 0.1,\n",
    "               attn_dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.mlp_size = mlp_size\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        \n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape = embedding_dim)\n",
    "        self.attn = MultiHeadAttention(embedding_dim=embedding_dim,\n",
    "                                       num_heads=num_heads,\n",
    "                                       dropout = attn_dropout\n",
    "                                      )\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.mlp = MLP(embedding_dim=embedding_dim,\n",
    "                       mlp_size=mlp_size,\n",
    "                       dropout=mlp_dropout)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # layer norm and attn with residual conn\n",
    "        x = x + self.attn(self.layer_norm1(x))\n",
    "\n",
    "        # layer norm and mlp with residual conn\n",
    "        x = x + self.mlp(self.layer_norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:57.502132Z",
     "iopub.status.busy": "2025-02-04T09:44:57.501842Z",
     "iopub.status.idle": "2025-02-04T09:44:57.518101Z",
     "shell.execute_reply": "2025-02-04T09:44:57.517319Z",
     "shell.execute_reply.started": "2025-02-04T09:44:57.502100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ViT\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size = 224,\n",
    "                 patch_size = 16,\n",
    "                 in_channels = 3,\n",
    "                 num_classes = 10,\n",
    "                 embedding_dim = 768,\n",
    "                 mlp_size = 3072,\n",
    "                 num_heads = 12,\n",
    "                 attn_dropout = 0.1,\n",
    "                 mlp_dropout = 0.1,\n",
    "                 num_transformer_layer = 12):\n",
    "        super().__init__()\n",
    "    \n",
    "        # patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size = img_size,\n",
    "            patch_size = patch_size,\n",
    "            in_channels = in_channels,\n",
    "            embedding_dim = embedding_dim\n",
    "        )\n",
    "    \n",
    "        # transformer encoder\n",
    "        self.transformer_encoder = nn.Sequential(*[\n",
    "            TransformerBlock(embedding_dim = embedding_dim,\n",
    "                            num_heads = num_heads,\n",
    "                            mlp_size = mlp_size,\n",
    "                            mlp_dropout = mlp_dropout)\n",
    "            for _ in range(num_transformer_layer)\n",
    "        ])\n",
    "    \n",
    "        # classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "            x = self.patch_embed(x)\n",
    "            x = self.transformer_encoder(x)\n",
    "            x = self.classifier(x[:,0]) # only cls token\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:57.519178Z",
     "iopub.status.busy": "2025-02-04T09:44:57.518942Z",
     "iopub.status.idle": "2025-02-04T09:44:58.594628Z",
     "shell.execute_reply": "2025-02-04T09:44:58.593717Z",
     "shell.execute_reply.started": "2025-02-04T09:44:57.519159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pass an image\n",
    "\n",
    "vit = ViT()\n",
    "vit(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T13:12:00.297101Z",
     "iopub.status.busy": "2025-02-04T13:12:00.296826Z",
     "iopub.status.idle": "2025-02-04T13:12:03.475774Z",
     "shell.execute_reply": "2025-02-04T13:12:03.475138Z",
     "shell.execute_reply.started": "2025-02-04T13:12:00.297079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "def train_step(model, dataloader, loss_fn, optimizer,\n",
    "              device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0,0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        y_pred = model(X)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += ((y_pred_class== y).sum().item())/len(y_pred)\n",
    "\n",
    "    train_loss = train_loss/len(dataloader)\n",
    "    train_acc = train_acc/len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def test_step(model, dataloader, loss_fn, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0,0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            test_pred = model(X)\n",
    "    \n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "            test_pred_label = torch.argmax(torch.softmax(test_pred, dim=1), dim=1)\n",
    "            test_acc += ((test_pred_label == y).sum().item())/len(test_pred_label)\n",
    "\n",
    "    test_loss = test_loss/len(dataloader)\n",
    "    test_acc = test_acc/len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "def train(model,\n",
    "         train_dataloader,\n",
    "         test_dataloader,\n",
    "         optimizer,\n",
    "         loss_fn,\n",
    "         epochs,\n",
    "         device):\n",
    "    results = {'train loss': [],\n",
    "              'train_acc': [],\n",
    "              'test_loss':[],\n",
    "              'test_acc':[]}\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model= model,\n",
    "                                          dataloader= train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer = optimizer,\n",
    "                                          device = device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                       dataloader= test_dataloader,\n",
    "                                       loss_fn = loss_fn,\n",
    "                                       device = device)\n",
    "\n",
    "        print(\n",
    "            f'epoch: {epoch+1} | '\n",
    "            f'train loss: {train_loss:.4f} | '\n",
    "            f'train acc: {train_acc:.4f} | '\n",
    "            f'test loss: {test_loss:.4f} | '\n",
    "            f'test acc: {test_acc:.4f} | '\n",
    "        )\n",
    "\n",
    "        results['train_loss'].append(train_loss)\n",
    "        results['train_acc'].append(train_acc)\n",
    "        results['test_loss'].append(test_loss)\n",
    "        results['test_acc'].append(test_acc)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T13:12:04.135044Z",
     "iopub.status.busy": "2025-02-04T13:12:04.134643Z",
     "iopub.status.idle": "2025-02-04T13:12:04.147646Z",
     "shell.execute_reply": "2025-02-04T13:12:04.146598Z",
     "shell.execute_reply.started": "2025-02-04T13:12:04.135018Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params = vit.parameters(),\n",
    "                            lr=0.0003,\n",
    "                            betas = (0.9,0.99),\n",
    "                            weight_decay = 0.5)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:58.810098Z",
     "iopub.status.busy": "2025-02-04T09:44:58.809805Z",
     "iopub.status.idle": "2025-02-04T09:44:58.823969Z",
     "shell.execute_reply": "2025-02-04T09:44:58.823076Z",
     "shell.execute_reply.started": "2025-02-04T09:44:58.810077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T09:44:58.825298Z",
     "iopub.status.busy": "2025-02-04T09:44:58.824975Z",
     "iopub.status.idle": "2025-02-04T10:18:53.754235Z",
     "shell.execute_reply": "2025-02-04T10:18:53.752910Z",
     "shell.execute_reply.started": "2025-02-04T09:44:58.825267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = train(model = vit,\n",
    "               train_dataloader = train_loader,\n",
    "               test_dataloader = test_loader,\n",
    "               optimizer = optimizer,\n",
    "               loss_fn = loss_fn,\n",
    "               epochs= 2,\n",
    "               device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-04T10:18:53.754947Z",
     "iopub.status.idle": "2025-02-04T10:18:53.755248Z",
     "shell.execute_reply": "2025-02-04T10:18:53.755140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(vit.state_dict(), \"vit_cifar10.pth\")\n",
    "print(\"Model saved as vit_cifar10.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
